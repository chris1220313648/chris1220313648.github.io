---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am Jiayi Chen, a graduate in Automation from Tongji University (Class of 2019). Currently, I am pursuing my Masterâ€™s in Computer Science and Technology at Harbin Institute of Technology (HIT). Meanwhile, I serve as a Research Assistant  at the IRPN Lab, Systems Hub, Hong Kong University of Science and Technology (Guangzhou), where my research focuses on Embodied Intelligence.

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# ğŸ”¥ Project

- Our GitHub repository([Embodied-AI-Paper-TopConf
](https://github.com/Songwxuan/Embodied-AI-Paper-TopConf)), which collects academic papers in the field of embodied intelligence, has garnered over 300 stars. 

- Our open-source project [LLaVA-VLA](https://github.com/OpenHelix-Team/LLaVA-VLA), a Vision-Language-Action model built upon the popular open-source VLM LLaVA, has received over 70 stars.
# ğŸ“ Publications 


<div class='paper-box'><div class='paper-box-image'><div><img src='images/ud-vla.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://irpn-eai.github.io/UD-VLA.github.io/)

**Jiayi Chen**, **Wenxuan Song**, **Pengxiang Ding**, **Ziyang Zhou**, **Han Zhao**, **Feilong Tang**, **Donglin Wang**, **Haoang Li**
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><img src='images/survey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TowardsaUnifiedUnderstanding of Robot Manipulation: A Comprehensive Survey](https://arxiv.org/abs/2510.10903)

Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Wei Zhao, Zhe Li, Pengxiang Ding, Cheng Chi, Haoang Li, Chang Xu, Xiaolong Zheng, Donglin Wang, Shanghang Zhang, Badong Chen
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><img src='images/flowvla_teaser.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[FlowVLA: Thinking in Motion with a Visual Chain of Thought](https://irpn-lab.github.io/FlowVLA/)

**Zhide Zhong**, **Haodong Yan**, **Junfeng Li**, **Xiangchen Liu**, **Xin Gong**, **Wenxuan Song**, **Jiayi Chen**, **Haoang Li**

</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><img src='images/ceed-vla.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1"> 
[CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding](https://arxiv.org/pdf/2506.13725)

**Wenxuan SongÂ¹\***, **Jiayi ChenÂ¹\***, **Pengxiang DingÂ²Ë’Â³â€ **, **Yuxin HuangÂ¹**, **Han ZhaoÂ²Ë’Â³**, **Donglin WangÂ²**,  **Haoang LiÂ¹â€¡**

<small>Â¹ IRPN Lab, HKUST(GZ) Â² MiLab, Westlake University Â³ Zhejiang University</small>  

<!-- **Wenxuan SongÂ¹\***, **Jiayi ChenÂ¹\***, **Pengxiang DingÂ²Ë’Â³**, **Han ZhaoÂ²Ë’Â³**, **Wei ZhaoÂ²**,**Zhide ZhongÂ¹**,  **Zongyuan Geâ´**, **Jun MaÂ¹**,  **Haoang LiÂ¹â€¡**

<small>Â¹ IRPN Lab, HKUST(GZ) Â² MiLab, Westlake University Â³ Zhejiang University â´ Monash University</small>   -->
<!-- <small>* Equal Contributionâ€ƒâ€ƒâ€  Project Leaderâ€ƒâ€ƒâ€¡ Corresponding Author</small> -->
- Wepropose CEED-VLA, a universal acceleration method for significant inference speedup while maintaining manipulation performance.
- We conduct a consistency distillation process to unlock the modelâ€™s capabilities of fast inference.
- We identify the inefficient iteration as the bottleneck of Jacobi decodingâ€™s speedup and propose the early-exit decoding to solve it.
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><img src='images/reconvla.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1"> 

[ReconVLA: Reconstructive Vision\textendash Language\textendash Action Model as Effective Robot Perceiver](https://zionchow.github.io/ReconVLA/)

**Wenxuan Song**, **Ziyang Zhou**, **Han Zhao**, **Jiayi Chen**, **Pengxiang Ding**, **Haodong Yan**, **Yuxin Huang**, **Feilong Tang**, **Donglin Wang**, **Haoang Li**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IROS2025</div><img src='images/PD-VLA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding](https://arxiv.org/pdf/2503.02310)


**Wenxuan SongÂ¹\***, **Jiayi ChenÂ¹\***, **Pengxiang DingÂ²Ë’Â³**, **Han ZhaoÂ²Ë’Â³**, **Wei ZhaoÂ²**,**Zhide ZhongÂ¹**,  **Zongyuan Geâ´**, **Jun MaÂ¹**,  **Haoang LiÂ¹â€¡**

<small>Â¹ IRPN Lab, HKUST(GZ) Â² MiLab, Westlake University Â³ Zhejiang University â´ Monash University</small>  
<!-- <small>* Equal Contributionâ€ƒâ€ƒâ€  Project Leaderâ€ƒâ€ƒâ€¡ Corresponding Author</small> -->

We propose parallel decoding framework for VLA models integrated with action chunking. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/rational-vla.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[RationalVLA: A Rational Vision-Language-Action Model with Dual System](https://arxiv.org/pdf/2506.10826)

**Wenxuan Song, Jiayi Chen, Wenxue Li, Xu He, Han Zhao, Can Cui, Pengxiang Ding, Shiyan Su, Feilong Tang,Donglin Wang, Xuelian Cheng, Zongyuan Ge, Xinhu Zheng, Zhe Liu, Hesheng Wang, Haoang Li**

<!-- <small>Â¹ IRPN Lab, HKUST(GZ) Â² MiLab, Westlake University Â³ Zhejiang University</small>   -->
<!-- <small>* Equal Contributionâ€ƒâ€ƒâ€  Project Leaderâ€ƒâ€ƒâ€¡ Corresponding Author</small> -->
- We introduce a new benchmark, the Rational Manipula-tion (RAMA) with defective instructions in 6 dimensions.
- We propose Rational Vision-Language-Action model (RationalVLA). It is a dual system for the robotic armto perceive the environment, handle unseen and defective instructions effectively, and demonstrate performant manipulation capabilities. 
</div>
</div>



<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

<!-- # ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

# ğŸ“– Educations
- *2023.06 - 2025.06 (now)*, Pursuing a Master's degree in Computer Science and Technology at Harbin Institute of Technology.
- *2019.09 - 2023.06*, Studied Automation at Tongji University and obtained a Bachelor's degree in Engineering.

<!-- # ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# ğŸ’» Internships
- *2024.2 - 2024.06*, IO Intelligence, China.